{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs funcionam como uma rede neural tradicional, tendo como diferença o uso de dois tipos de entrada: a entrada atual (X) e as entradas anteriormente processadas pela rede (h), tendo cada uma seu vetor de pesos e bias. Assim, para implmementar uma camada RNN, são necessários como etapa:\n",
    "* Inicializar cada peso aleatoriamente\n",
    "* Leitura e processamento da base\n",
    "* Multiplicar pesos e entradas do vetor X\n",
    "* Multiplicar pesos e entradas do vetor h\n",
    "* Junção aditiva formada pelas multiplicações das etapas anteriores\n",
    "* Função de ativação tanh para camadas escondidas\n",
    "* Calcular saída da rede\n",
    "\n",
    "![alt text](imgs/LSTM3-SimpleRNN.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar Pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10 \n",
    "seq_length = 5\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 \n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 \n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 \n",
    "bh = np.zeros((hidden_size, 1)) \n",
    "by = np.zeros((vocab_size, 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura e processamento do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Capacitacao LAVID - LSTM E RNN\n",
      "\n",
      "data has 45 characters, 25 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_rnn(X,h,Wxh,Whh,Why,bh,by):\n",
    "    j_aditiva = np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x)\n",
    "    h = np.tanh(j_aditiva + bh)\n",
    "    y = softmax(np.dot(Why, h) + by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Para implementar LSTM, a essência é a mesma, a rede terá uma entrada X atual e as entradas anteriores h, cada uma com seus respectivos pesos e bias. Entretanto, a rede divide-se em quatro camadas principais tidas como forget gate, input e update gate, output gate. Cada camada possui uma implementação diferenciada para filtrar os dados da rede e considerar o que é importante para o aprendizado. Assim, tem-se como etapas:\n",
    "* Iniciar aleatoriamente os pesos e bias de cada camada\n",
    "* Multiplicar cada entrada individualmente pelo peso Wf (peso da camada forget gate), somando-a em uma junção aditiva e passando pela função de ativação sigmoid e adicionando-se o bias dessa camada\n",
    "* Multiplicar cada entrada individualmente pelo peso Wi (peso da camada input gate), somando-a em uma junção aditiva e passando pela função de ativação sigmoid e adicionando-se o bias dessa camada\n",
    "* Multiplicar cada entrada individualmente pelo peso Wu (peso da camada update gate), somando-a em uma junção aditiva e passando pela função de ativação tanh e adicionando-se o bias dessa camada\n",
    "* Atualizar a memória da célula multiplicando a memória anterior pela camada forget gate\n",
    "* Atualizar a memória da célula somando-se os resultados obtidos pelas entradas input e update gate\n",
    "* Multiplicar cada entrada individualmente pelo peso da camada de saida Wo, somando-a em uma junção aditiva e passando pela função de ativação sigmoid\n",
    "* Atualizar a memória da célula com os valores de saída\n",
    "* Calcular a saída y da célula\n",
    "\n",
    "![alt text](imgs/LSTM3-chain.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Ativação e predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wf = np.random.randn(hidden_size, hidden_size+vocab_size)\n",
    "Wi = np.random.randn(hidden_size, hidden_size+vocab_size)\n",
    "Wo = np.random.randn(hidden_size, hidden_size+vocab_size)\n",
    "Wc = np.random.randn(hidden_size, hidden_size+vocab_size)\n",
    "Wy = np.random.randn(vocab_size,hidden_size)\n",
    "bf = np.random.randn(hidden_size,1)\n",
    "bi = np.random.randn(hidden_size,1)\n",
    "bo = np.random.randn(hidden_size,1)\n",
    "bc = np.random.randn(hidden_size,1)\n",
    "by = np.random.randn(vocab_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_lstm(X,h,Wf,bf,Wi,bi,Wu,bu,c_prev):\n",
    "    # Gates\n",
    "    forget_gate = sigmoid(np.dot(Wf,X)+np.dot(Wf,h)+bf)\n",
    "    input_gate  = sigmoid(np.dot(Wi,X)+np.dot(Wi,h)+bi)\n",
    "    update_gate = np.tanh(np.dot(Wu,X)+np.dot(Wu,h)+bu)\n",
    "    \n",
    "    #Atualiza celula\n",
    "    c_next = forget_gate*c_prev+update_gate*input_gate\n",
    "    output = sigmoid(np.dot(Wo,X)+bo)\n",
    "    cell = output*np.tanh(c_next)\n",
    "    \n",
    "    # Compute prediction of the LSTM cell (≈1 line)\n",
    "    y = softmax(np.dot(Wy, cell) + by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Flatten, Dropout, Embedding,  BatchNormalization\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vocab_size, input_length=sequence_length))\n",
    "model.add(LSTM(10, return_sequences=True,\n",
    "               input_shape=(sequence_length, len(data))))  \n",
    "model.add(Dropout(0.9))               \n",
    "model.add(LSTM(10, return_sequences=True))  \n",
    "model.add(LSTM(10))  \n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "classes = model.predict(x_test)\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
