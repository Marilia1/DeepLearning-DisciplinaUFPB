{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neurais Utilizando o Tensorflow\n",
    "\n",
    "[TensorFlow™](https://www.tensorflow.org) é uma biblioteca de software livre utilizada para operações numéricas que requerem alta performance como Deep Learning. Sua estrutura simbólica de grafos (tensors) permite uma arquitetura flexível e otimizada para diferentes plataformas (e.g. CPUs, GPUs, dispositivos móveis), além de facilitar a implementação de redes em diferentes níveis de complexidade. \n",
    "\n",
    "Utilizando o tensorflow em Deep Learning, podemos definir o data flow da rede, compreendendo o funcionamento da mesma sem nos preocuparmos em calcular as derivadas da mesma de forma otimizada por exemplo. Isso é possível através da estrutura de tensors, em que cada operação representa um nó do grafo (e.g. adição, multiplicação) e é executada por uma seção. Além da seção, definimos placeholders e variáveis. Placeholders podem ser entendidos como as entradas e saídas da minha rede, objectos que não seráo alterados durante o treinamento. Variáveis por outro lado, podem ser tidas como pesos e bias, ou seja, objetos que são atualizados a cada interação do treinamento.       \n",
    "\n",
    "Com essa estrutura, pode-se ter um controle do código e do que acontece na rede, podendo alterar alguns pontos chave como o algoritmo de otimização e a forma de utilizar as variáveis, sendo mais indicado o uso para aplicações e pesquisas que requerem um estudo mais aprofundado de Deep Learning. Quando o objetivo é a utilização em alto nível, uma boa opção é utilizar o Keras, framework que utiliza o tensorflow como backend em um nível de abstração mais alto.\n",
    "\n",
    "Para maior contextualização do tensorflow puro, esse notebook irá mostrar como implementar os exercícios da Lista de Exercícios de Redes Neurais da disciplina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1 - Definir as entradas e saídas da rede\n",
    "\n",
    "Para o tensorflow, as entradas e saídas da rede são definidas a partir de Placeholders. A função initPlaceholders(input_shape, output_shape) implementa essas estruturas, sendo necessário definir o tipo da variável (float 32 para esse caso) e a dimensão. \n",
    "\n",
    "A dimensáo de entrada pode ser tida como a quantidade de instâncias contidas na base de treinamento e da quantidade necessária de entradas/saídas para processar. Por exemplo, em um XOR, tem-se 4 possíveis instâncias cada uma com duas entradas, dessa forma, a dimensão do placeholder de entrada será [4,2]. Da mesma forma, um placeholder de saída para o XOR contém um array [4,1], sendo 4 a quantidade de instâncias que serão classificadas e 1 o tamanho do vetor de saída para cada instância (0 ou 1). Por fim, é boa prática também definir o nome do placeholder (X ou Y para esse exemplo) afim de identificá-lo no grafo formado pelo tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[4,2], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, shape=[4,1], name = 'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2 - Inicializar os pesos\n",
    "\n",
    "Além de inicializar os placeholders, no tensorflow também é necessário definir as variáveis, que para redes neurais podem ser pesos e bias. Em uma rede neural, cada camada possui seus respectivos pesos e bias. Dessa forma, cada peso tem dimensão igual a quantidade de entradas da camada e da quantidade de entradas da camada seguinte. Por exemplo, a camada de entrada possui pesos de dimensão igual a [input_num, hidden_num], em que input_num é a quantidade de entradas e hidden_num a quantidade e entradas da camada seguinte. Da mesma forma, uma camada escondida possui pesos na dimensão [hidden_num, output_num], sendo hidden_num a quantidade de entradas da camada atual e output_num a quantidade de entradas da camada de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random_uniform([2,2], -1, 1), name=\"w1\")\n",
    "w2 = tf.Variable(tf.random_uniform([2,1], -1, 1), name=\"w2\")\n",
    "    \n",
    "b1 = tf.Variable(tf.zeros([4,2]), name=\"b1\")\n",
    "b2 = tf.Variable(tf.zeros([4,1]), name=\"b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3 - Definir a estrutura da rede\n",
    "\n",
    "Após inicializar os pesos, pode-se definir a estrutura da rede. Isso pode-ser feito a partir da equação:\n",
    "\n",
    "camada = função de ativação (X * W + b)\n",
    "\n",
    "Cada camada pode ter uma função de ativação de acordo com o problema, para redes neurais tradicionais, é usual utilizar a função sigmoid, linear ou degrau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.sigmoid(tf.add(tf.matmul(X, w1),b1)) #camada escondida\n",
    "y_pred = tf.sigmoid(tf.add(tf.matmul(hidden,w2),b2)) #camada de saída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 4 - Otimização\n",
    "\n",
    "Definida a estrutura da rede, a saída y_pred precisa ser comparada com a saída original Y da base de dados para calcular a perda (loss) do sistema. Uma forma simples de calcular a perda é calcular o erro médio quadrático (MSE), embora normalmente utilize-se a cross-entropy.\n",
    "\n",
    "Após o cálculo da perda, pode-se corrigir o erro calculado a partir de um otimizador que possui como parâmetro uma taxa de aprendizado que pode ter o valor entre 0 e 1. Existem também outros otimizadores utilizados no [tensorflow](https://www.tensorflow.org/versions/master/api_guides/python/train), sendo o Adam bastante utilizado para Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = tf.reduce_mean(tf.squared_difference(y_pred, Y)) \n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=Y)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 5 - Conjunto de treinamento\n",
    "\n",
    "Para esse notebook, o conjunto de treinamento é a porta XOR, correspondente aos valores da Figura 1.\n",
    "![alt text](imgs/xor_truthtable.png)\n",
    "\n",
    "Ao representar graficamente as entradas e saídas, percebe-se pela Figura 2 que o problema não é linearmente separável, náo podendo ser representado por um perceptron. Por esse motivo, o XOR é um problema clássico para redes neurais multicamadas, sendo solucionado nesse notebook a partir de uma camada escondida.\n",
    "![alt text](imgs/xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y_train = [[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 6 - Treinamento\n",
    "\n",
    "Definida a rede, pode-se treinar a partir da inicialização das variáveis do tensorflow e da definiça2o da seção sess. Feito isso, a rede é treinada a partir de um número de interações que pode ser definido empiricamente e o treino é realizado com o método run, que recebe como parâmetro o otimizador calculado na etapa 5 e feed_dict. O feed_dict recebe as instâncias de entrada e de saída para cada interação.\n",
    "Resumidamente, pode-se dizer que o processo de treinamento de uma rede é feito a partir dos passos:\n",
    "1. Inicializar os placeholders\n",
    "2. Inicializar variáveis de pesos e bias\n",
    "3. Definir estrutura da rede (quantidade de camadas, função de ativação)\n",
    "4. Calcular a perda (loss) a partir de uma função para calculo do erro como a MSE\n",
    "5. Corrigir a perda calculada na etapa 4 a partir do gradiente\n",
    "6. Definir conjunto de treinamento\n",
    "7. Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs\", sess.graph) #tensorboard\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    sess.run(optimizer, feed_dict={X: X_train, Y: Y_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 7 - Resultado\n",
    "\n",
    "Com a rede treinada, pode-se visualizar o y_pred, pesos, bias e a perda da rede. Uma vez que a rede utiliza uma função de ativação de saída sigmoid, os valores retornados serão entre 0 e 1, enquanto a saída do XOR conforme Figura 1 é binária. Uma forma comum de tornar a saída da rede também binária é definir um limiar (comumente igual a 0.5) e, se os valores de y_pred forem superiores ao limiar retornará True, ou False caso contrário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[ True]\n",
      "[ True]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "for result in sess.run(y_pred, feed_dict={X: X_train, Y: Y_train}):\n",
    "    print(result>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 8 - Visualizando a rede\n",
    "\n",
    "No Tensorflow é possível visualizar a rede criada a partir do Tensorboard. Para fazer isso é necessario apenas definir um escritor (writer) do grafo criado, ir ao terminal e digitar tensorboard --logdir /caminho/do/arquivo e, por fim, acessar no navegador a partir do endereço http://localhost:6006. A Figura 3 ilustra o grafo para esse notebook, podendo-se visualizar todas as operações realizadas.\n",
    "\n",
    "![alt text](imgs/tensorboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
