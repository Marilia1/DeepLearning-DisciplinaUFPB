{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "\n",
    "Redes Convolucionais (CNNs) possuem arquiteturas para diversos problemas de classificação, entretanto, existem aplicações em que torna-se necessário detectar e/ou segmentar objetos. Baseado nisso, tem-se como problema de detecção aquele em que é necessário identificar não apenas o que é um dado objeto na imagem (classificar), como também identificar a posição do mesmo na imagem. Além disso, tem-se como problema de segmentação aquele em que deseja-se classificar cada pixel da imagem individualmente. A Figura abaixo ilustra duas formas de segmentação, a semântica (i) e a por instância (ii), em que (i) não diferencia objetos conjuntos de uma mesma classe e (ii) detecta cada objeto individualmente para segmentar.\n",
    "\n",
    "![alt text](imgs/segmentation.png \"Title\")\n",
    "\n",
    "Com isso, para utilizar CNNs com segmentação, pode-se classificar cada pixel indivualmente, mas o custo seria consideravelmente elevado. Assim, uma solução é utilizar [Redes Completamente Convolucionais](https://arxiv.org/pdf/1411.4038.pdf\"), ilustradas na Figura abaixo. Nesse tipo de rede, pode-se utilizar uma arquitetura base de classificação (e.g. VGG-16) e substituir as camadas completamente conectadas por camadas convolucionais, de forma que a camada de saída da rede possua dimensões iguais as de entrada em termos de H X W, mas com o número de canais igual a quantidade de classes da base.\n",
    "\n",
    "![alt text](imgs/fcn.png \"Title\")\n",
    "\n",
    "Dessa forma, esse notebook ilustra a implementação de uma Rede Completamente Convolucional (FCN) utilizando Keras para segmentação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitetura VGG-16\n",
    "\n",
    "Como base da rede, esse notebook utilizará uma arquitetura VGG-16, ilustrada na Figura abaixo, utilizando-se apenas as suas camadas convolucionais. Essa arquitetura possui cinco blocos principais contendo, em cada um, camadas convolucionais e de MaxPooling. \n",
    "\n",
    "Dessa forma, o primeiro bloco é formado por duas camadas convolucionais de 64, enquanto o segundo é formado por duas camadas convolucionais de 128 filtros, o terceiro por três camadas convolucionais de 256 filtros e os dois últimos por três camadas convolucionais de 512 filtros cada um. Todos os blocos possuem uma última camada de saída MaxPooling e as camadas convolucionais possuem filtros de dimensão 3x3.\n",
    "\n",
    "![alt text](imgs/17.png \"Title\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_16(img_input):\n",
    "    ### primeiro bloco\n",
    "    model = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "    model = Conv2D(64, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = MaxPooling2D((2, 2), strides=(2, 2))(model)\n",
    "    \n",
    "    ### Segundo bloco\n",
    "    model = Conv2D(128, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(128, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = MaxPooling2D((2, 2), strides=(2, 2))(model)\n",
    "    \n",
    "    ### Terceiro bloco\n",
    "    model = Conv2D(256, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(256, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(256, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(model)\n",
    "    \n",
    "    ### Quarto bloco\n",
    "    model = Conv2D(512, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(512, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(512, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = MaxPooling2D((2, 2), strides=(2, 2))(model)  \n",
    "    \n",
    "    ### Quinatto bloco\n",
    "    model = Conv2D(512, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(512, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = Conv2D(512, (3, 3), activation='relu', padding='same')(model)\n",
    "    model = MaxPooling2D((2, 2), strides=(2, 2))(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN-32\n",
    "\n",
    "Após implementar os cinco blocos convolucionais da VGG, torna-se necessário substituir as camadas completamente conectadas por camadas convolucionais de forma que a dimensão de saída da rede possua os mesmos H e W da imagem de entrada, mas com a quantidade de canais igual a quantidade de classes.\n",
    "\n",
    "Assim, no método fcn32, a primeira camada após os blocos da vgg é constituido por uma rede convolucional com filtros 7x7 e 512 filtros. Após isso, a camada convolucional de 4096 neurônios é substituído por uma camada convolucional de 4096 filtros de dimensão 1x1. Feito isso, pode-se adicionar uma última camada para classificar, com a quantidade de filtros igual a quantidade de classes do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn32(input_img):\n",
    "    # VGG-16\n",
    "    model = vgg_16(input_img)\n",
    "\n",
    "    ### Substituir camadas FC por Convolucionais\n",
    "    model = Conv2D(512, (7, 7), activation='relu', padding='same')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Conv2D(4096, (1, 1), activation='relu', padding='same')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    #camada final\n",
    "    model = Conv2D(21, (1, 1), activation='linear', padding='valid', strides=(1, 1))(model)\n",
    "    ### upsampling\n",
    "    model = BilinearUpSampling2D(size=(32, 32))(model)\n",
    "    model = Model(img_input, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: FCN-16 e FCN-8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
